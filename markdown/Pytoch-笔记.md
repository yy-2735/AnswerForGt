# OVERVIEW

1.基本步骤
* Dataset：准备数据集
* Model：设计模型，构造损失函数和优化器
* Training
* Inferring

2.**过拟合**：
机器学习中常出现的问题，模型过度学习了训练数据中的噪声、异常值和细节，导致对新数据的泛化能力下降。

3.**泛化**：
模型在未见过的新数据上的表现能力
>**泛化误差**是评估模型泛化能力的指标，它表示模型在新数据上的预测误差。

4.**维度诅咒**：
feature（特征维度，每个样本或数据点通常由一组特征组成，而特征维度表示这组特征的数量。）越多，需要的样本量越多，即**随着维度的增加，数据空间的体积指数级增加。**
>eg.考虑一个房屋价格预测的问题。每个房屋可以由多个特征来描述，如房屋的面积、卧室数量、浴室数量、地理位置等。如果我们使用这些特征来训练一个机器学习模型，那么特征维度就表示这些特征的数量，比如4个特征，即特征维度为4。
eg.若feature=1-一维空间，若feature=2-二维空间，行和列都要采样...由此可得，若feature=n-n维空间，样本量=x的n次方


>可以考虑以下办法
特征选择和特征提取：通过选择最相关的特征或将高维特征映射到低维空间，可以减少特征的数量和维度，以提高模型的性能。
维度约简：使用降维技术如主成分分析（PCA）、线性判别分析（LDA）等，将高维数据映射到低维空间，以保留数据的主要信息同时减少维度。
数据预处理：对数据进行归一化、标准化等预处理操作，以确保各个维度的数据具有相似的尺度和重要性。
领域知识的应用：利用对问题领域的了解，选择合适的特征和数据表示形式，以减少维度对模型性能的影响。

5.**几种算法：**
* 穷举法
* 贪心法：是一种常见的算法设计思想，用于在每个步骤中做出局部最优的选择，以期望最终能够达到全局最优解。在每个决策点上，贪心算法总是选择当前看起来最好的选项，而不考虑后续决策可能带来的影响。
* 分治法：是一种算法设计策略，通过将问题**分解**为较小的子问题，然后**解决**这些子问题，并将它们的解**合并**起来，最终得到原始问题的解。
* 动态规划

6.辅助思路：
代码用于构建计算图

7.**广播：**
* 在进行广播操作时，系统会自动调整张量的形状，使得它们能够逐元素地进行运算。
* 许多例子里，x,y都以矩阵形式输入

8.通常使用转置的原因：
将维度放在前面

9.Mini-batch（小批量）：
是深度学习中用于训练模型的一种常见的数据处理方式。在训练过程中，将训练数据划分为多个小批量，并分批输入到模型中进行参数更新。



***
# 数学公式的代码表示：
1.**定义**函数

    def function_name(parameters):
        return expression

 * function_name 是你给函数起的**名称**，用于在其他地方调用该函数。
 * parameters 是函数的**参数**，即函数接受的输入。参数可以是可选的，也可以有**多个**。
 * 函数体是由缩进的代码块组成，表示函数执行的具体操作。
 * return 语句用于指定函数的**返回值**。可以选择性地使用return 语句，如果没有指定 return，函数将默认返回 None。

2.一个循环语句:
```for x, y in zip(xs, ys): ```
* 用于同时迭代两个可迭代对象 xs 和 ys 中的元素。
具体来说，**zip(xs, ys) 是一个函数调用**，它将两个可迭代对象 xs 和 ys 中对应位置的元素进行**配对**，返回一个生成器对象，每次迭代时生成**一对元素**。
x 和 y 分别表示每一对配对的元素。
* xs 和 ys 是两个列表，每个列表都有n个元素。通过使用 zip 函数将它们配对，我们可以在循环中同时遍历 xs 和 ys，并在每次迭代时将配对的元素赋值给 x 和 y。
* eg.
    ``` 
    xs = [1, 2, 3]
    ys = [4, 5, 6]
    for x, y in zip(xs, ys):
    print(x, y)```
* 输出结果为1 4，2 5，3 6

3.命名的潜规则：
>y_pred:
* "y"（表示"预测"）和 "pred"（表示"预测结果"）组合而成的。
* 一般用于表示模型的预测输出结果，无论是分类任务还是回归任务。
>_val
* 表示验证集（validation set）相关的指标或变量。



4.**平方误差的计算公式：**
   `cost += (y_pred - y) ** 2`
* +=表示将平方误差**累加**到 cost 变量上，可以逐步计算总体成本

5.**循坏语句：**
    `for epoch in range(100)`
* 其中epoch是一个循环变量，用于迭代范围内的每个值。
range(100)表示一个从0到99的整数序列，表示循环将执行100次，每次迭代时epoch的值会依次为0、1、2、...、99。

6.**返回值计算**
    `cost_val=cost(x_data,y_data)`
* cost(x_data, y_data)：这是一个函数调用表达式，调用了名为cost的函数，并将x_data和y_data作为实参，**输入**给该函数，并返回一个代表损失或成本的数值。
* cost_val：这是一个变量名，用于**存储**函数cost的返回值。

7.**链式求导**：
* forward:前馈运算，得到（输出z）和（输入x，权重w）的函数
* backward：反馈运算，得到loss对于z的偏导，再得到（loss）对（x,w）的导数
<img src="/markdown/picture/p-1.jpg">
<img src="/markdown/picture/p-2.jpg">
<img src="/markdown/picture/p-3.jpg">

8.**选择权重：**
    `w=torch.Tensor([1.0]) w.requires_grad=Ture`
* w.requires_grad = True是设置了张量的requires_grad属性为True，这意味着我们希望对该张量进行**自动求导**

9.**反向传播：**
    `l=loss(x,y) l.backward`
* 假设一个损失函数 loss，并且已经通过前向传播计算出了模型的预测结果 y_pred 和真实标签 y，并将它们传递给了损失函数，即 l = loss(y_pred, y)。
* 通过调用 l.backward() 方法，可以**自动计算**损失函数 l 相对于所有具有 requires_grad=True 的张量的梯度。从损失函数开始，沿着计算图**反向传播**梯度
    `w.data=w.data-0.01*w.grad.data`
* 梯度储存在grad中，是张量，所以要转换为data数据类型来防止生成计算图
* 计算图生成次数过多会占用内存
* PS.使用 .data 属性进行参数更新是一种操作，它绕过了计算图的梯度跟踪机制，因此需要谨慎使用。这种操作可能会破坏自动求导的正确性，导致梯度计算错误或不完整。


10.清零：
    `w.grad.data.zero()`
* 将权重清零，以免得到w1+w2+...的结果
* 默认不清零

11.定义**模型**

    class LinearModel(torch.nn.Module):
    def __init__(self, input_size, output_size):
        super(LinearModel, self).__init__()
        self.linear = nn.Linear(input_size, output_size)

    def forward(self, x)://表示x可以调用
        return self.linear(x)

* LinearModel 类是一个自定义的线性模型类
* torch.nn.Module： 是一个基类，通过创建一个继承自 torch.nn.Module 的类，可以定义自己的模型
* __init__(self, input_size, output_size)：构造**函数**，用于初始化模型的参数。
在这个例子中，我们传递了类的实例本身（self）输入**维度** (input_size) 和输出**维度** (output_size)。
* super(LinearModel, self) ：**必有**，表示**调用 LinearModel 类的父类**（或超类）的方法。通过调用 __init__() 方法，我们可以确保在子类的构造函数中执行父类的初始化操作。
* self.linear = nn.Linear(input_size, output_size) 的意思是在 LinearModel 类的构造函数中创建一个名为 self.linear 的**线性层**，并将其**赋值**给类的实例变量 self.linear。
* 没有backward的原因：在 PyTorch 中，当你定义了一个**继承**自 torch.nn.Module 的类，并使用模型的输出计算损失函数时，**backward 方法会自动被调用**，实现了自动求导

13.计算和优化误差

    criterion=torch.nn.MSLoss(size_average=False)
    optimizer=torch.optim.SGD(model.parameters(),lr=0.01)

* torch.nn.MSLoss :计算**均方误差**的损失函数类。
* size_average=False : 参数设置,通过将其设置为 False，表示不对每个样本的损失值进行**平均**，而是对所有样本的损失值进行求和。
* optimizer=torch.optim.SGD(model.parameters(),lr=0.01):优化器类
* SGD：是一种常用的优化算法，用于更新模型参数以最小化损失函数。
* model.parameters() :传递给优化器的**参数列表**。
* lr=0.01 :参数设置，它指定了学习率（learning rate）。

14.训练过程中**更新参数**：

    optimizer.zero_grad()  # 清除之前的梯度
    loss.backward()  # 计算梯度
    optimizer.step()  # 更新参数




***

# 线性模型
1.模型最基本的方式
>所以，在机器学习中，常先取一个线性模型看是否有效，若无效再调整/更换模型

2.MSE：均方误差（Mean Squared Error）的缩写,MSE = (1/n) * Σ(yᵢ - ŷᵢ)²,MSE 的值越小表示模型的预测结果与真实值之间的差异越小，即模型的性能越好。

3.绘制最小损失曲线
ps.图像的可视化很重要

4.激活函数：
* 对每一层的**输出**加一个**非线性**的变化函数
* 如果没有激活函数，多层神经网络无论多深都可以**被简化**为等效于单层线性模型。通过引入非线性激活函数，神经网络可以学习非线性的特征映射，从而更好地适应复杂的数据分布和解决非线性问题。
* 常见的激活函数
>Sigmoid函数：将输入映射到0到1之间的连续输出，具有平滑的S形曲线。
双曲正切（Tanh）函数：将输入映射到-1到1之间的连续输出，与Sigmoid函数类似但在原点附近更敏感。
ReLU函数（Rectified Linear Unit）：将负值映射为零，而正值保持不变，具有简单的阈值特性。
Leaky ReLU函数：与ReLU函数类似，但在负值区域引入一个小的斜率，以解决ReLU函数的死亡神经元问题。
**Softmax函数**：用于多分类问题，将输入映射为概率分布，使得所有输出在0到1之间且总和为1



# 梯度下降算法
1.随机猜测：w=random value
2.更新权重的方法：
>新权重 = 旧权重 - 学习率 * 梯度

* 其中：旧权重是当前的权重，**学习率（步长）**，梯度是损失函数关于权重的**偏导数**。
* 需要注意的是：权重的更新是迭代过程，需要多次重复计算和更新，**直到达到某个停止条件**，如达到最大迭代次数或损失函数**收敛**（如梯度为0）。通过不断地更新权重，模型可以逐渐优化和拟合训练数据，以提高预测的准确性和泛化能力。
* 体现：贪心法，具有**局部**最优选择性（如在非凸函数中，在局部最优解时**已经收敛**，无法达到全局最优）
* 鞍点:指在目标函数的优化过程中出现的一种特殊情况。在鞍点处，**梯度为零**，但该点既不是局部最小值也不是局部最大值，而是一个高维空间中的**平坦区域**(联系图像去理解)
另：局部最优和鞍点的比较：
>局部最优问题:是指优化算法在搜索过程中陷入一个局部最小值而无法找到全局最小值的情况。在存在多个局部最小值的函数中，优化算法可能会被吸引到一个局部最优解，并无法进一步探索其他更优的解。解决局部最优问题的方法包括使用不同的初始值、尝试不同的优化算法、使用随机性的算法等。
鞍点问题相对更具挑战性，因为鞍点不仅仅是在梯度为零的点上，而且在梯度为零的点上**具有正曲率和负曲率**.在鞍点处，优化算法可能会停滞，因为梯度下降方向没有指向更好解的明确信号。在高维空间中，鞍点是非常常见的，并且存在于许多复杂的优化问题中。

3.随机梯度下降
>简要步骤：

* 初始化模型参数：在开始训练之前，需要随机初始化模型的权重和偏置。
* 随机选择样本：从训练数据中**随机选择一个**样本作为当前的训练样本。
* 计算梯度：使用当前训练样本计算模型在**该样本**上的损失函数关于模型参数的梯度。
* 更新参数：
* 重复步骤2-4：重复执行随机选择样本、计算梯度和更新参数的步骤，直到达到预定的停止条件。
>与普通梯度下降的对比：
* 计算效率高
* 更好的泛化能力
* 可以处理大型数据集
* 参数更新的不稳定性
* 学习率的选择困难：SGD对学习率非常敏感。选择过大的学习率可能导致参数在参数空间中跳过最优解，而选择过小的学习率可能导致学习过程缓慢。
* 可能陷入局部极值点：由于SGD的随机性，它有时可能陷入局部极值点而无法找到全局最优解。

# 具体实现
1.x,y的维度已知，即可求出w,b的维度
2.loss必须是一个**标量**，才能使用backward

# 逻辑斯蒂回归
1.不合适线性模型的原因：
* 分类问题：计算输出的是**概率**
2.数据集下载
3.二分类问题
4.
* 目的：要将实数值**映射**到【0，1】之间
* 实现：Sigmoid函数（Logistic函数）,将yhat带入，得到概率值
* 代码：`sigmoid(x) = 1 / (1 + exp(-x))`
<img src="/markdown/picture/p-4.jpg">
<img src="/markdown/picture/p-5.jpg">
5.当**两个分布**尽量接近时，模型的预测概率与真实标签概率之间的差异较小，导致较小的损失函数值和梯度，从而减小误差


# 处理多维特征的输入
1.关系表中：一行叫记录；一列叫特征
<img src="/markdown/picture/p-6.jpg">
2.上标是样本，下标是维度（特征）
<img src="/markdown/picture/p-7.jpg">
3.经观察，可以把左侧方程合并为右侧的形式
（即将z合并为向量形式）
<img src="/markdown/picture/p-8.jpg">
PS.输入的维度是8维，而输出的维度为1
5.多个线性变换首尾相连构成多层神经网络
6.矩阵实际上体现了维度变化的映射，故可以把其视为一**空间变化的函数**
7.转化为向量形式可以提高计算的效率
8.我们希望通过多个线性变化层，通过找到最优权重将他们组合，变成一个非线性变换函数。
* 所以神经网络也可以说成转换为非线性的过程

9.一般来说，隐层越多，非线性变换的学习能力就越强（可能会过度学习噪声）

# 加载数据集
1.dataset：构造数据集
dataloader:拿出mini-batch供训练时使用
2.训练过程的参数；
<img src="/markdown/picture/p-9.jpg">
* epoch:时期，是深度学习中表示模型训练过程的一个单位。一个 epoch 表示模型已经在整个训练集上进行了一次完整的训练。
* batch-size:大小，是指在训练过程中，每个小批量中包含的样本数量。
* literation:迭代，是指在深度学习训练过程中参数更新的次数。**一个**迭代包括了一次前向传播、计算损失函数、反向传播、梯度更新的过程。
* 迭代的数量通常由数据集的大小和批量大小决定。例如，如果数据集包含1000个样本，每个迭代使用大小为100的批量，那么需要进行10次迭代才能完成一次完整的训练（也就是一个epoch）。
3.dataloader的生成过程：
<img src="/markdown/picture/p-10.jpg">


***
# 数据集的划分方法
1.留出集（Hold-out Set）：
* 简介：将数据集划分为训练集和验证集两个部分。
* 示例代码：

        from sklearn.model_selection import train_test_split
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

* train_test_split：是scikit-learn库中的一个函数，用于将数据集划分为训练集和验证集。
* 具体来说，train_test_split函数的参数如下：
>X：特征向量，包含了输入数据的特征。
y：目标变量，包含了对应于特征向量的目标值。
test_size：验证集的比例，可以是一个浮点数（0~1之间）或整数。如果是**浮点数**，表示验证集在整个数据集中的比例；如果是**整数**，表示验证集的样本数量。
random_state：随机数种子，用于控制数据划分的随机性。设置相同的种子将确保每次运行代码时得到相同的划分结果。

* 返回值是四个数组：
>X_train：训练集的特征向量。
X_val：**验证集**的特征向量。
y_train：训练集的目标变量。
y_val：验证集的目标变量

* 随机数种子（Random Seed）：是一个初始值，用于确定随机数生成器的起始状态。
在机器学习中，随机数种子的**作用**是控制随机性，通过设置相同的随机数种子，使得随机过程在每次运行时产生**相同的结果。**
设置随机数种子不是必须的，除非你想要确保实验的**可重复性**，或者希望与**他人共享**代码时得到相同的结果，或者当进行模型调优时，通过固定随机数种子可以更好地比较不同超参数组合之间的性能差异。
在scikit-learn等机器学习库中，随机数种子通常作为函数的参数进行设置，例如在`train_test_split`函数中的`random_state`参数。你可以根据需要选择是否设置随机数种子，或者使用默认的随机种子（通常为None或不设置）以获得每次运行都不同的结果。
实际上，你可以选择**任何整数值**作为随机数种子，通常并没有绝对的**对错之分**，只要你在不同的实验中使用相同的种子值，就能获得相同的结果。

* 在示例代码中的实现
>test_size=0.2：表示将数据集划分为80%的训练集和20%的验证集。
random_state=42是设置的随机数种子，确保每次运行代码时得到相同的划分结果。

2.K折交叉验证（K-fold Cross Validation）：

* 简介：将数据集划分为**K个互斥的子集**，重复K次训练和验证过程，以获得平均性能指标。
* 示例代码：

        from sklearn.model_selection import cross_val_score, KFold
        kfold = KFold(n_splits=5, shuffle=True, random_state=42)
        scores = cross_val_score(estimator, X, y, cv=kfold)

3.自助采样法（Bootstrap Sampling）：
* 简介：通过有放回地从原始数据集中随机选择样本创建新的训练集。
* 示例代码：

        from sklearn.utils import resample
        X_train_boot, y_train_boot = resample(X, y, n_samples=len(X), replace=True, random_state=42)

4.时间序列划分（Time Series Split）：
* 简介：根据时间顺序将时间序列数据集划分为训练集和测试集。
* 示例代码：

        from sklearn.model_selection import TimeSeriesSplit
        tscv = TimeSeriesSplit(n_splits=5)
        for train_index, test_index in tscv.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

# 多分类问题
1.实际上是**二分类**问题的变式，只有唯一正确的类别**输出1**，其余全都**输出0**
2.即：各输出值必须要满足**弥散分布**的要求,也是要解决的**难点**：
* 所有概率>=0（涉及到负值的转化）
* 所有概率总和为1
3.实现方式：
<img src="/markdown/picture/p-11.jpg">
<img src="/markdown/picture/p-12.jpg">
4.eg.
<img src="/markdown/picture/p-13.jpg">
***
# Tensor （张量）
1.基本定义：
* 数据类型，用于**存储和操作**数据，并在深度学习模型中进行计算和训练。
* 两个重要组成部分：
> data:保存权重
grad：损失函数对权重的导数

2.具有以下好处

>高效的数值计算
自动求导
灵活的维度处理：Tensor变量支持多维数组，并且提供了丰富的维度操作和索引方式。
GPU加速
丰富的函数和操作

3.转化为张量的方式：
* 使用 torch.tensor() 函数：

        import torch
        data = [1, 2, 3, 4, 5]  # 示例数据
        tensor_data = torch.tensor(data)

4.张量的基本运算

    # 张量加法
    x = torch.tensor([1, 2, 3])
    y = torch.tensor([4, 5, 6])
    z = x + y
    # 张量乘法
    a = torch.tensor([2, 4, 6])
    b = torch.tensor([3, 3, 3])
    c = a * b
    # 张量矩阵乘法
    m1 = torch.tensor([[1, 2], [3, 4]])
    m2 = torch.tensor([[5, 6], [7, 8]])
    m3 = torch.matmul(m1, m2)
    import torch
    # 改变张量形状
    x = torch.tensor([[1, 2, 3], [4, 5, 6]])
    y = x.view(3, 2)
    # 转置张量
    z = x.t()
    # 改变张量维度
    a = torch.tensor([1, 2, 3, 4])
    b = a.unsqueeze(0)  # 在第0维插入维度
    c = b.squeeze()  # 去除所有维度为1的维度
    import torch
    # 广播张量
    x = torch.tensor([1, 2, 3])
    y = torch.tensor([4, 5, 6])
    z = x + y
    # 广播张量和标量
    a = torch.tensor([1, 2, 3])
    b = 2
    c = a + bv

***
# 优化算法
1.SGD
* SGD (Stochastic Gradient Descent) 是一种常用的优化算法，用于训练机器学习模型，特别是在深度学习中广泛使用。它是一种基于梯度的迭代优化算法，旨在最小化损失函数并更新模型的参数。
* SGD 的基本思想是通过迭代地计算训练样本的损失函数梯度来更新模型参数。每次迭代中，SGD 从训练集中随机选择一个样本（或者一小批样本），计算该样本的损失函数梯度，并根据梯度的方向和大小来更新模型参数。这种随机选择样本的方式使得 SGD 非常高效，并且能够处理大型数据集。