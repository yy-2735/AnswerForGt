# 深度学习
1.人工智能的实现方法
2.以神经网络为出发点
3.神经网络可以实现：接受人提供的数据后，从网络关系中自己学习并理解
***
# 神经网络的思想
## 从神经网络过渡到数学模型
1.要素：输入，阈值，输出
2.输入按**加权**计算
3.偏置
4.技巧：增加虚拟输入

## 阶层性神经网络
1.<img src="/markdown/picture/阶层级神经网络.png">
2.按照**层**来划分神经单元
3.各层的功能：
* 输入层：负责读取给予神经网络的信息。属于这个层的神经单元没有输入箭头，只是将从数据得到的值原样输出。
* 隐藏层：执行信息处理操作 (1) 和 (2)，实际处理信息。
* 输出层：执行信息处理操作 (1) 和 (2)，显示计算结果

4.全连接层：前一层的神经单元与下一层的所有神经单元都有箭头连接

## 深度学习
1.叠加了很多层的神经网络
2.卷积神经网络是叠加的一种方法

## 具体例子
<img src="/markdown/picture/1.4例题.png">

## 重要的隐藏层
1.对识别困难的问题，给出解决方案有：**由神经单元之间的关系强度给出答案**
2.简单理解为：
<img src="/markdown/picture/1.5-1.png">
<img src='/markdown/picture/1.5-2.png'>
* 会影响“恶魔”，使其不能正确传递信息
* 对“恶魔”的影响情况看恶魔的“心的偏置”
？为什么2，11不兴奋--为什么最外面一圈不算在模式中
4.“恶魔”的交情：就是神经单元的权重
5.“心的偏置”：原本不希望做出反应的神经单元，因此需要禁止这样的信号并使信号变清晰，这样的功能就是偏置。

## 恶魔的人数：
1.某种预估
<img src="/markdown/picture/1.6-1.png">

## 从数学角度看神经网络的学习
1.有监督学习：为了确定神经网络的权重和偏置，事先给予数据，这些数据称为学习数据。根据给定的学习数据确定权重和偏置，**称为学习**。
2.学习的过程：计算神经网络得出的预测值与正解的误差，确定使得误差总和达到最小的权重和偏置。这在数学上称为模型的**最优化**。
3.代价函数：针对全部学习数据，计算预测值与正解的**误差的平方**（称为平方误差），然后再相加得到的误差总和。
PS.利用平方误差确定参数的方法在数学上称为**最小二乘法**（它在统计学中是回归分析的常规手段）
***
# 神经网络的数学基础
## 神经网络所需的函数
1.特殊地：多变量
2.Signmoid函数
* 性质：
>可导
有界
单调递增

ps.由性质此函数值可以用概率来解释
* 激活函数的代表
3.正态分布的概率密度函数
* 公式：
<img src="/markdown/picture/2.1-1.png">

* 作用：
按照正态分布产生的随机数称为正态分布随机数。在神经网络的计算中，经常用到正态分布随机数作为初始值。

## 有助于理解神经网络的数列和递推关系式
1.递推关系式：在神经网络的世界中，所有神经单元的**输入和输出**在数学上都可以认为是用联立递推式联系起来的。
2.是误差反向传播法的基础
3.计算机擅长**递推关系式**

## 有助于理解神经网络的向量基础
1.<img src="/markdown/picture/2.4-1.png">
* 当两个向量相反时取最小值，这是**梯度下降法**的基本原理
* 内积表示两个向量在多大程度上指向相同方向（可以理解为**相似度**）这是**卷积神经网络**的基础
2.神经网络要处理多空间，所以考虑用向量去表示（eg.加权输入可以表示为内积的形式）
3.张量：
<img src="/markdown/picture/2.4-2.png">

## 有助于理解神经网络的矩阵基础
1.<img src="/markdown/picture/2.5-1.png">

## 神经网络的导数基础
1.是最优化不可或缺的方法
2.常用**分数形式**来表示
3.**导数的非线性**是误差反向传播法的关键
4.特殊地，<img src="/markdown/picture/2.6-1.png">
5.最小值的**必要**条件：x=a处导函数为0
PS.该性质有可能成为梯度下降法的障碍

## 神经网络的偏导数基础
1.因为构成神经网络的神经单元的权重和偏置都被作为变量处理，所以处理多变量函数的导数很重要
2.<img src="/markdown/picture/2.7-1.png">
3.关于**某个特定变量**的导数就称为偏导数
4.符号表示
<img src="/markdown/picture/2.7-2.png">
5.最小值
<img src="/markdown/picture/2.7-3.png">
6.拉格朗日乘数法
<img src="/markdown/picture/2.7-4.png">
即三个步骤
* 引入参数
* 构建函数
* 求偏导

## 误差反向传播法必需的链式法则
1.单变量函数的复合函数求导，也称为链式法则：
<img src="/markdown/picture/2.8-1.png">
<img src="/markdown/picture/2.8-2.png">
ps.平方的时候不成立
* eg.
<img src="/markdown/picture/2.8-3.png">
2.多变量
* 必须对**全部**变量都应用链式法则
* eg.<img src="/markdown/picture/2.8-4.png">
* eg.<img src="/markdown/picture/2.8-5.png">

## 梯度下降法的基础： 多变量函数的近似公式
1.单变量函数的近似公式
<img src="/markdown/picture/2.9-1.png">
理解：即切线斜率*x改变量=y改变量

2.多变量函数的近似公式
<img src="/markdown/picture/2.9-2.png">

3.近似公式的向量表示：
<img src="/markdown/picture/2.9-3.png">

4.泰勒展开式：
<img src="/markdown/picture/2.9-4.png">

## 梯度下降的含义和公式
1.是寻找**最小值**的方法
2.可被总结为 *一边慢慢地移动位置一边寻找陡坡* 的操作
<img src="/markdown/picture/2.10-1.png">
* 理解：取最值的条件从相切线变为**相切面**
<img src="/markdown/picture/2.10-2.png">

3.近似公式和内积的关系
由前面的推导有：
<img src="/markdown/picture/2.10-3.png">

4.梯度下降法的数学基础：
* **b**=-k**a**
* 应用：
<img src="/markdown/picture/2.10-4.png">
<img src="/markdown/picture/2.10-5.png">

5.推广：<img src='/markdown/picture/2.10-6.png'>

6.哈密顿算子：一种简易的表现方式
<img src='/markdown/picture/2.10-7.png'>
同理有：位移向量
<img src='/markdown/picture/2.10-8.png'>

7.<img src='/markdown/picture/2.10-9.png'>

## 用Excel体验梯度下降法
## 最优化问题和回归分析
1.回归分析：着眼于其中一个特定的变量，用其余的变量来解释这个特定的变量。
2.eg.
<img src="/markdown/picture/2.12-1.png">
<img src='/markdown/picture/2.12-2.png'>

3.代价函数：
* 即误差总和
* 除了平方误差的总和外，还存在其他形式

4.
<img src="/markdown/picture/2.12-3.png">
