1.
* 机器学习是旨在使计算机系统能够在**没有明确指令**的情况下依靠**自学习和推理**从大量历史**数据**中识别模式并执行任务的技术。简单来说，机器学习通过算法和统计模型，使计算机系统能够**根据历史数据来预测结果。**
* 而深度学习是机器学习的一个**子集**，是一种更高级的机器学习技术。他们的具体区别有：
>数据类型：机器学习通常处理结构化数据，而深度学习更适用于非结构化数据。
学习方法：机器学习使用算法和模型进行学习，而深度学习使用**神经网络**进行预测和决策。
训练过程：机器学习通常需要人工标记数据集，而深度学习则通过**神经网络**进行训练。
硬件需求：深度学习通常需要更大的数据集和更高性能的计算资源。
特性：机器学习解决方案通常具有更易于解释的特性，而深度学习更加复杂，难以解释。
适用场景：机器学习适用于结构化数据和对解释性要求较高的场景，而深度学习则适用于非结构化数据和对准确性要求较高的场景。

2.
监督学习和无监督学习是机器学习中两种重要的学习方式。其中：
* 监督学习：模型从带有**标签**的训练样本中学习，并**根据这些标签预测**新的实例（即据给定的学习数据确定权重和偏置）。
>训练（学习）数据包括输入和相应的输出（标签）。模型通过学习这些输入和输出之间的关系来进行预测。监督学习的目标是通过从**已知示例**中进行学习，推断出隐藏在数据中的模式和规律。


* 无监督学习：模型从**未标记**的训练样本中学习，并**自动发现**数据中的模式和结构。
>在无监督学习中，训练数据只包括输入，**没有相应的输出或标签**（即没有已知示例）。模型通过分析输入数据的特征和相似性来进行**聚类、降维或生成新样本等任务**。无监督学习的目标是在没有明确指导的情况下，从数据中挖掘出有用的信息。

* 一言以蔽之，**我认为**监督学习是有中生有，无监督学习是无中生有。

3.
* 偏导数：关于**某个特定变量**的导数
>作用：因为构成神经网络的神经单元的权重和偏置都被作为变量处理，所以会处理**多变量函数**的导数很重要，而偏导数正是处理多元函数的一种方式，可用于实现**最优化损失函数的梯度**。

* 链式法则：即**复合函数求导**法则
>同上，也为处理多元函数的一种方式，用于**辅助计算梯度**，是误差反向传播法必备的。

* 梯度：向量，它由函数的偏导数组成，表示目标函数相对于模型参数的**变化率**。
>通过计算梯度，我们可以确定目标函数在当前参数值处的最陡增长方向，从而帮助我们**调整参数以最小化目标函数**或优化模型。

* 矩阵乘法：A的行与B的列对应相乘
>用于计算每一层的输入和权重之间的线性组合，以及激活函数的输入。通过多个矩阵乘法和激活函数的组合，神经网络可以学习复杂的非线性关系。矩阵乘法还用于计算损失函数相对于模型参数的梯度，以进行反向传播和参数更新。

> 高效的数值计算

4.
* 损失函数:
是用于衡量模型预测输出与实际之间差异的函数。
常见的损失函数包括均方误差、交叉熵损失等
* 梯度下降:
其基本原理是:通过计算函数关于模型参数的**梯度**（导数），并沿着梯度的**反方向更新**参数，**直到达到某个停止条件**，如达到最大迭代次数或损失函数**收敛**（如梯度为0），以达到最小化。可以被形象地理解为：*一边缓慢移动位置一边寻找陡坡*,即最速下降原理。需要注意的是，梯度下降法**局部最优解**的体现
为什么最速下降可以使得函数值最小化呢？这可以通过函数的局部导数性质来解释。**在接近函数最小值的地方，函数的导数（梯度）趋近于零，而在导数为负的地方，函数值下降最快**。因此，通过朝着梯度的反方向移动，可以使函数值逐渐下降并接近最小值。
* 反向传播：
应用链式法则，将整个网络中每个参数的梯度表示为以该参数为起点的梯度链。通过**沿着这个链更新每个参数**，我们可以根据损失函数的变化来调整网络的权重和偏置项。

5.
* 样本：
是指数据集中的单个**实例**。一个样本可以是一个数据点、一个观测记录或一个输入-输出对。样本通常由一组特征组成。
* 特征：
用来描述样本或观测的**属性**。它们是输入数据的不同方面或维度的表示。例如，在鸢尾花数据集中，花的特征可以包括花瓣长度、花瓣宽度、萼片长度和萼片宽度。每个样本都由一组特征表示，这些特征用来描述样本的属性或特征。
* 引入激活函数的原因有：
>**引入非线性**：激活函数通过引入非线性变换，使得神经网络可以学习非线性关系。如果没有激活函数，多层神经网络将等效于单层线性模型，无法捕捉数据中的非线性结构。
激活神经元：激活函数将神经元的输入加权和转换为输出，通过激活函数的设置，可以控制神经元的激活程度。这对于神经网络的表达能力和模型的灵活性至关重要。
梯度传播：激活函数对于反向传播算法中的梯度计算和梯度传播起着重要的作用。激活函数的导数用于计算参数梯度，从而进行参数更新。

6.
* 线性回归：
是一种用于建立**连续**输出变量与输入特征之间线性关系的回归分析方法。
它的基本原理是通过拟合一个**线性模型**来预测输出变量的值。线性回归假设输入特征与输出变量之间存在线性关系，并通过最小化平方损失函数来估计模型参数。

逻辑回归：
是一种用于建立**二分类或多分类问题**的概率模型。它的基本原理是使用**逻辑函数**（如 Sigmoid 函数）将线性模型的输出映射到概率值，并通过最大似然估计或交叉熵损失函数来估计模型参数。
逻辑回归假设输入特征与输出的对数几率之间存在线性关系，并**使用逻辑函数将线性模型的输出转换为类别概率**（如task1）。在二分类问题中，逻辑回归可以用于预测样本属于某一类别的概率。

* 相同点：
都可以使用梯度下降等优化算法来估计模型参数。
* 区别：
>输出类型：线性回归用于预测连续输出变量，而逻辑回归用于预测二分类或多分类问题。
输出转换：线性回归直接输出连续值，逻辑回归通过逻辑函数将线性输出转换为概率值。
损失函数：线性回归通常使用平方损失函数，逻辑回归通常使用最大似然估计或交叉熵损失函数。
模型表达能力：线性回归假设输入特征与输出之间的关系是线性的，逻辑回归假设输入特征与输出的对数几率之间是线性的。


7.
* 将数据集划分为训练集、验证集和测试集是为了**评估**机器学习模型的性能和进行**模型选择**。这种划分有助于模型的泛化能力评估和调优。
* 
> 训练集：训练集是用于模型参数的学习和训练的数据集。模型通过观察和**学习**训练集中的样本和标签之间的关系来调整自身的参数，以便能够对未知数据做出准确的预测。训练集通常是最大的数据集，占据总数据集的大部分比例。
 验证集（开发集）:验证集用于**调整**模型的超参数和进行模型选择。超参数是指模型训练过程中需要手动设置的参数，如学习率、正则化参数等。通过在验证集上评估不同超参数设置下模型的性能，可以选择最佳的超参数配置。验证集的目的是提供一个独立于训练集的数据集来**评估**模型的性能，并进行模型的选择和调优。
 测试集：测试集用于**评估最终**模型的性能和泛化能力。测试集是模型在训练和调优过程中从未见过的数据，用于模拟模型在实际应用场景中遇到的新数据。通过在测试集上进行评估，可以获取对模型在真实情况下的性能的客观评估。测试集的结果应该是最终模型性能的最终指标，并用于评估模型的泛化能力。

* 三者的区别和联系
>训练集和验证集（开发集）通常来自同一个数据分布，但是在划分时是独立的。（测试集）应与（训练集和验证集）**完全独立**，来模拟模型在真实环境中的表现，且测试集的结果**不被告知**。
训练集和验证集的划分比例可以根据具体任务和数据集大小来确定，常见的比例是70-80%的数据用于训练，10-15%的数据用于验证。测试集的大小应足够大以提供对模型性能的可靠估计。
* 划分不正确导致的问题：
>过拟合：如果训练集和验证集划分不合理，模型可能在训练集上表现良好，但在验证集上表现较差。这可能是因为模型过度关注了训练集中的**噪声**或特定样本，而**无法泛化**到新数据。
欠拟合：如果验证集过小或与训练集相似度过高，模型可能无法充分学习数据的模式和规律，导致性能较差。

* 正确划分和使用数据集的方法包括：

>随机划分：确保数据集的划分是随机的，以避免样本的偏斜或相关性问题。
合理比例：根据数据集的大小和任务需求，合理划分训练集、验证集和测试集的比例。（常用的方法有：留出集划分）
交叉验证：除了简单的划分，还可以使用交叉验证方法，（如K折交叉验证），将数据集划分为多个子集进行训练和验证，以更准确地评估模型性能。
* 总之，正确的数据集划分和使用可以提供对模型性能的可靠评估，帮助选择最佳模型和超参数配置，并提高模型在实际应用中的泛化能力。

8.
* Softmax函数：
将输入映射为概率分布，使得所有输出在0到1之间且总和为1
Softmax函数的定义如下：
给定一个实数向量 z = [z₁, z₂, ..., zₖ]，Softmax函数将向量 z 的每个元素转化为一个介于 0 和 1 之间的实数，并确保这些转化后的值的总和为 1。Softmax函数的表达式如下：
softmax(zᵢ) = exp(zᵢ) / (∑ⱼ exp(zⱼ))
其中，exp(x) 表示 e 的 x 次方，∑ 表示求和运算。

* 主要应用
>**多类别分类**：在神经网络中，Softmax函数通常作为最后一层的激活函数，将输出转化为概率分布，用于预测样本属于每个类别的概率。
**交叉熵损失函数**：在多类别分类任务中，常使用交叉熵损失函数作为目标函数。Softmax函数与交叉熵损失函数结合使用，可以度量预测概率分布与真实标签之间的差异，并通过梯度下降等优化算法来调整模型参数。
（集成学习中的权重分配：在集成学习中，Softmax函数可以用来分配不同模型的权重。通过将模型的预测概率转化为权重，可以按照模型的表现来动态调整模型的重要性。）
（生成对抗网络 (GAN)：在生成对抗网络中，Softmax函数经常用于判别器网络的最后一层，将网络的输出转化为生成样本为真实样本的概率。）